{
  "research_question": "Does LoRA preserve model internal representations better than full fine-tuning on text classification tasks?",
  "answer": "Yes, but only at scale: Strong evidence for large datasets (>60K samples), no evidence for small datasets (<5K samples)",
  "overall_drift_reduction_percent": 9.865313625064008,
  "tasks_analyzed": 3,
  "statistically_significant_tasks": 1,
  "key_insight": "LoRA representation preservation benefit is dataset-scale dependent with threshold ~60K samples",
  "dataset_size_effect": "Positive correlation between dataset size and drift reduction",
  "performance_trade_off": "Minimal (<0.5% accuracy cost for SST-2, parity for MRPC/RTE)",
  "metrics_analyzed": [
    "CKA",
    "Cosine Similarity",
    "Accuracy",
    "F1 Score"
  ],
  "visualization_files": [
    "results/drift_analysis/drift_reduction_by_task.png",
    "results/drift_analysis/layer_wise_drift_all_tasks.png",
    "results/drift_analysis/drift_difference_heatmap.png",
    "results/drift_analysis/cosine_similarity_all_tasks.png",
    "results/drift_analysis/dataset_size_vs_drift.png",
    "results/drift_analysis/drift_vs_performance.png"
  ],
  "implications": {
    "continual_learning": "LoRA superior for large-task sequential learning",
    "multi_task": "LoRA provides stable representations across large tasks",
    "small_task_learning": "No representation preservation advantage",
    "practical_guidance": "Use LoRA for tasks >60K samples to preserve base knowledge"
  }
}