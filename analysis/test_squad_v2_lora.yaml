best_hyperparameters:
  learning_rate: 0.00026550072443850685
  lora_alpha: 16
  lora_dropout: 0.07346584755343696
  lora_r: 8
  num_train_epochs: 4
  per_device_train_batch_size: 16
  warmup_ratio: 0.15326172301909433
  weight_decay: 0.04591324341992395
expected_performance: 0.0
method: lora
optimization_summary:
  n_completed: 1
  n_pruned: 0
  n_trials: 1
task: squad_v2
