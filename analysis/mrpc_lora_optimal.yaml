best_hyperparameters:
  learning_rate: 0.0002542488962759577
  lora_alpha: 64
  lora_dropout: 0.1965000235957656
  lora_r: 8
  num_train_epochs: 3
  per_device_train_batch_size: 16
  warmup_ratio: 0.00998559623942603
  weight_decay: 0.0030318693862057544
expected_performance: 0.82
method: lora
optimization_summary:
  n_completed: 10
  n_pruned: 0
  n_trials: 10
task: mrpc
