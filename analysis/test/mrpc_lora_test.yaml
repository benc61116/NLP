best_hyperparameters:
  learning_rate: 9.585514245464014e-06
  lora_alpha: 64
  lora_dropout: 0.005305549217145278
  lora_r: 16
  num_train_epochs: 5
  per_device_train_batch_size: 4
  warmup_ratio: 0.19044212417422154
  weight_decay: 0.09682220631310112
expected_performance: 0.48
method: lora
optimization_summary:
  n_completed: 2
  n_pruned: 0
  n_trials: 2
task: mrpc
