best_hyperparameters:
  learning_rate: 0.0004400509491405891
  lora_alpha: 16
  lora_dropout: 0.29280860580811074
  lora_r: 32
  num_train_epochs: 5
  per_device_train_batch_size: 16
  warmup_ratio: 0.2290179149331798
  weight_decay: 0.0696117803358321
expected_performance: 0.51
method: lora
optimization_summary:
  n_completed: 20
  n_pruned: 0
  n_trials: 20
task: squad_v2
