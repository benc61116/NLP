optimal_hyperparameters:
  full_finetune:
    expected_performance: 0.92
    hyperparameters:
      learning_rate: 5.5627080403346613e-05
      num_train_epochs: 4
      per_device_train_batch_size: 1
      warmup_ratio: 0.1694315312960467
      weight_decay: 0.08197129194236585
    optimization_summary:
      n_completed: 10
      n_pruned: 0
      n_trials: 10
  lora:
    expected_performance: 0.7266666666666667
    hyperparameters:
      learning_rate: 0.00012167612358620479
      lora_alpha: 32
      lora_dropout: 0.0005795114987926176
      lora_r: 4
      num_train_epochs: 2
      per_device_train_batch_size: 8
      warmup_ratio: 0.05423585159287157
      weight_decay: 0.051340329212135154
    optimization_summary:
      n_completed: 10
      n_pruned: 0
      n_trials: 10
optimization_efficiency: 67% faster
optimization_method: optuna_tpe_optimized
task: sst2
total_trials: 20
trials_per_method: 10
