optimal_hyperparameters:
  full_finetune:
    expected_performance: 0.68
    hyperparameters:
      learning_rate: 9.662725253044474e-06
      num_train_epochs: 4
      per_device_train_batch_size: 1
      warmup_ratio: 0.0854944020516992
      weight_decay: 0.0176370230835036
    optimization_summary:
      n_completed: 10
      n_pruned: 0
      n_trials: 10
  lora:
    expected_performance: 0.64
    hyperparameters:
      learning_rate: 0.00040114258852550313
      lora_alpha: 32
      lora_dropout: 0.22670115279994313
      lora_r: 8
      num_train_epochs: 3
      per_device_train_batch_size: 8
      warmup_ratio: 0.11198847499353282
      weight_decay: 0.025598711056396842
    optimization_summary:
      n_completed: 10
      n_pruned: 0
      n_trials: 10
optimization_efficiency: 67% faster
optimization_method: optuna_tpe_optimized
task: rte
total_trials: 20
trials_per_method: 10
