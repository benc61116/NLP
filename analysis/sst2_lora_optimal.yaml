best_hyperparameters:
  learning_rate: 0.00012167612358620479
  lora_alpha: 32
  lora_dropout: 0.0005795114987926176
  lora_r: 4
  num_train_epochs: 2
  per_device_train_batch_size: 8
  warmup_ratio: 0.05423585159287157
  weight_decay: 0.051340329212135154
expected_performance: 0.7266666666666667
method: lora
optimization_summary:
  n_completed: 10
  n_pruned: 0
  n_trials: 10
task: sst2
