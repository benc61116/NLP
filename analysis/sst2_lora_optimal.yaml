best_hyperparameters:
  learning_rate: 0.0003230175187384698
  lora_alpha: 64
  lora_dropout: 0.258219174976903
  lora_r: 16
  num_train_epochs: 6
  per_device_train_batch_size: 16
  warmup_ratio: 0.24243611386932507
  weight_decay: 0.06334037565104235
expected_performance: 0.88
method: lora
optimization_summary:
  n_completed: 10
  n_pruned: 0
  n_trials: 10
task: sst2
