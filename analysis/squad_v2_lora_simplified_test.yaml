best_hyperparameters:
  learning_rate: 1.0253509690168497e-05
  lora_alpha: 8
  lora_dropout: 0.055021352956030146
  lora_r: 4
  num_train_epochs: 4
  per_device_train_batch_size: 4
  warmup_ratio: 0.2852142919229748
  weight_decay: 0.0731993941811405
expected_performance: 0.1
method: lora
optimization_summary:
  n_completed: 2
  n_pruned: 0
  n_trials: 2
task: squad_v2
