# NLP Research Project Configuration
# Comparing LoRA vs Full Fine-tuning on Llama-2-1.3B

# Model Configuration
model:
  name: "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"  # 1.1B Llama-2 architecture, no auth needed
  max_length: 512
  device_map: "auto"
  dtype: "bfloat16"  # Use bfloat16 for better gradient stability
  max_memory_percent: 95  # Use 95% of GPU memory - balanced efficiency
  classification_max_length: 512
  qa_max_length: 768

# LoRA Configuration
lora:
  r: 8                     # Optimal rank (higher ranks don't help)
  alpha: 16               # Optimal scaling
  dropout: 0.05           # Dropout for LoRA layers
  target_modules:         # Optimal module targeting
    - "q_proj"
    - "v_proj" 
  task_type: "CAUSAL_LM"
  bias: "none"

# Training Configuration
training:
  # General settings
  seed: 42
  gradient_checkpointing: true
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  
  # Batch sizes (adjust based on GPU memory)
  per_device_train_batch_size: 1  # Reduced for memory efficiency
  per_device_eval_batch_size: 2   # Reduced for memory efficiency  
  gradient_accumulation_steps: 4  # Reduced to help with gradient stability
  
  # Learning rates (reduced to prevent gradient explosion)
  learning_rate: 0.0002      # Higher for LoRA
  full_finetune_learning_rate_classification: [0.000003, 0.000005]  # Classification tasks (MRPC, SST-2, RTE) - REDUCED
  full_finetune_learning_rate_qa: [0.000002, 0.000003]  # QA tasks (SQuAD v2) - REDUCED (matches remote fix)
  full_finetune_learning_rate: 0.000005  # Default full fine-tuning LR - REDUCED
  
  # Optimization
  weight_decay: 0.01
  warmup_ratio: 0.1  # Increased to 0.1 for better training stability and gradient control
  lr_scheduler_type: "cosine"
  optim: "adamw_torch"
  max_grad_norm: 0.3  # VERY AGGRESSIVE gradient clipping for stronger gradient explosion prevention
  
  # Training duration
  num_train_epochs: 3
  max_steps: -1            # -1 means use num_train_epochs
  
  # Evaluation
  eval_strategy: "steps"
  eval_steps: 100
  save_strategy: "steps"
  save_steps: 500
  logging_steps: 50
  
  # Representation extraction settings (enabled for drift analysis)
  extract_representations_every_steps: 100  # Extract every 100 steps for drift analysis
  save_final_representations: true         # Save final representations for drift analysis
  extract_base_model_representations: true # Extract base model representations (should be VM3 only)
  
  # Early stopping
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  early_stopping_patience: 3

# Task-Specific Configuration
tasks:
  mrpc:
    name: "MRPC Paraphrase Detection"
    type: "classification"
    num_labels: 2
    metric: "f1"
    max_samples_train: null  # null means use all data
    max_samples_eval: null
    
  sst2:
    name: "SST-2 Sentiment Analysis"
    type: "classification" 
    num_labels: 2
    metric: "accuracy"
    max_samples_train: null
    max_samples_eval: null
    
  rte:
    name: "RTE Textual Entailment"
    type: "classification"
    num_labels: 2
    metric: "accuracy"
    max_samples_train: null
    max_samples_eval: null
    
  squad_v2:
    name: "SQuAD v2.0 Question Answering"
    type: "question_answering"
    num_labels: null
    metric: "f1"
    max_samples_train: 10000  # Limit for computational efficiency
    max_samples_eval: 2000

# Sanity Check Configuration
sanity_check:
  enabled: true
  num_samples: 10          # Small number for overfitting test
  max_epochs: 10           # More epochs for harder tasks
  expected_loss_threshold: 0.1  # Loss should drop below this
  
  # Task-specific learning rate multipliers for sanity checks
  # Based on task complexity and empirical testing
  task_multipliers:
    mrpc: 4                # Simple paraphrase detection - conservative
    sst2: 5                # Sentiment classification - moderate  
    rte: 6                 # Textual entailment - more complex reasoning
    squad_v2: 10           # QA + answerability - most complex, matches working commit
    
  # Fallback multiplier for unknown tasks
  default_multiplier: 4
  
# Infrastructure Configuration
infrastructure:
  gpu_memory_fraction: 0.9 # Use 90% of GPU memory
  mixed_precision: true    # Enable automatic mixed precision
  compile: false          # PyTorch 2.0 compilation (can be unstable)
  
# Weights & Biases Configuration
wandb:
  project: "NLP"  # Default - will be overridden by WANDB_PROJECT environment variable
  entity: "galavny-tel-aviv-university"
  save_code: true
  log_model: "all"        # Log model checkpoints
  watch_model: true       # Watch gradients and parameters
  log_freq: 50            # How often to log within epochs
  
  # Tags and grouping
  tags:
    - "llama-2-1.3b"
    - "lora-vs-full-ft"
    - "multi-task"
  
  # Experiment organization
  group_name: "lora-comparison-study"  # Group related experiments
  job_type: "experiment"               # Job type for organization

# Reproducibility Configuration  
reproducibility:
  seed: 42
  deterministic: true      # Make training deterministic (may be slower)
  benchmark: false        # Don't use CUDNN benchmark for reproducibility
  
# Phase Configuration (for distributed execution)
phases:
  phase1:
    description: "Sanity checks and baseline validation"
    tasks: ["sanity_check_all"]
    
  phase2a:
    description: "LoRA fine-tuning on all tasks"
    tasks: ["mrpc", "sst2", "rte", "squad_v2"]
    method: "lora"
    
  phase2b: 
    description: "Full fine-tuning on all tasks"
    tasks: ["mrpc", "sst2", "rte", "squad_v2"] 
    method: "full"

# Resource Allocation (3 GPU VMs)
vm_allocation:
  vm1:
    tasks: ["mrpc", "sst2"]
    gpu_memory: "24GB"
    description: "Classification tasks"
    
  vm2: 
    tasks: ["rte", "squad_v2"]
    gpu_memory: "24GB" 
    description: "Entailment and QA tasks"
    
  vm3:
    tasks: ["sanity_check", "analysis"]
    gpu_memory: "24GB"
    description: "Validation and analysis"

# Output Configuration
output:
  base_dir: "./results"
  save_predictions: true
  save_model_checkpoints: true
  create_comparison_plots: true
  
# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "./logs/experiment.log"
