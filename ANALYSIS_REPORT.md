# LoRA vs Full Fine-Tuning: Comprehensive Analysis Report

**Project:** Comparing Parameter-Efficient Fine-Tuning Methods on Text Classification  
**Model:** TinyLlama-1.1B (Llama-2 architecture)  
**Tasks:** MRPC, SST-2, RTE (GLUE benchmark)  
**Hardware:** NVIDIA L4 GPU (24GB VRAM)  
**Date:** October 5, 2025

---

## Executive Summary

This study provides a comprehensive comparison of LoRA (Low-Rank Adaptation) versus Full Fine-Tuning for text classification tasks, investigating two key research questions:

1. **RQ1:** Does LoRA preserve internal representations better than full fine-tuning?
2. **RQ2:** What is the deployment latency penalty for multi-adapter setups vs merged models?

### Key Findings

‚úÖ **RQ1 - Representational Drift:**
- **Task-Dependent Effect**: LoRA shows 29% less drift on SST-2, but no advantage on MRPC/RTE
- **Not Universal**: Representational preservation depends on task characteristics
- **High Directional Preservation**: LoRA maintains 0.97-0.99 cosine similarity with base model

‚úÖ **RQ2 - Deployment Efficiency:**
- **32.9% Latency Penalty**: LoRA is significantly slower than Full FT (p < 0.001)
- **Minimal Multi-Adapter Overhead**: <3% additional latency for adapter swapping
- **Comparable Memory**: Runtime memory usage is similar (~2GB each)
- **Throughput**: Full FT achieves 32.9% higher requests/second

### Bottom Line

**LoRA offers a nuanced trade-off rather than universal advantages:**
- ‚úÖ Benefits: Storage efficiency, multi-task flexibility, task-dependent stability
- ‚ùå Costs: Inference latency penalty, no universal drift advantage
- üéØ Recommendation: Choose based on deployment scenario (single vs multi-task)

---

## Research Question 1: Representational Drift

### Methodology

**Drift Measurement:**
- **Centered Kernel Alignment (CKA)**: Captures structural similarity between representations
- **Cosine Similarity**: Measures directional alignment with base model
- **Comparison**: Base model ‚Üí Fine-tuned model representations (all 11 transformer layers)

**Experimental Design:**
- 3 tasks (MRPC, SST-2, RTE) √ó 2 methods (LoRA, Full FT) √ó 3 seeds = 18 models
- 500 samples per task for representation extraction
- Paired statistical tests with Bonferroni correction

### Results

#### 1.1 Main Finding: Task-Dependent Drift Reduction

| Task | LoRA Mean CKA | Full FT Mean CKA | Difference | p-value | Interpretation |
|------|---------------|------------------|------------|---------|----------------|
| **SST-2** | 0.359 ¬± 0.009 | 0.101 ¬± 0.018 | **+255%** | **<0.001** | **‚úÖ Significant improvement** |
| MRPC | 0.235 ¬± 0.003 | 0.223 ¬± 0.003 | +5.5% | 0.025 | Slight advantage |
| RTE | 0.399 ¬± 0.003 | 0.398 ¬± 0.003 | +0.2% | 0.400 | No significant difference |

**Key Insight:**  
LoRA's representational advantage is **task-specific**, not universal. SST-2 (sentiment analysis) benefits dramatically (255% higher CKA), MRPC shows slight advantage (5.5%), while RTE shows virtually no difference.

**Note on CKA interpretation:** Higher CKA values indicate stronger similarity to the base model (i.e., less drift). CKA ranges from 0 (completely different) to 1 (identical representations).

#### 1.2 Directional Preservation (Cosine Similarity)

| Task | LoRA Cosine | Full FT Cosine | Interpretation |
|------|-------------|----------------|----------------|
| MRPC | 0.984 | 0.965 | Both high, LoRA slightly better |
| SST-2 | 0.975 | 0.774 | LoRA much stronger preservation |
| RTE | 0.996 | 0.995 | Both extremely high, nearly identical |

**Average:** LoRA maintains 0.97-0.99 cosine similarity (8-14¬∞ angular deviation), indicating strong directional alignment with pretrained knowledge.

#### 1.3 Statistical Robustness

**Negative Control (Seed-to-Seed Variance):**
- LoRA variance: 0.0002-0.0018 (very stable)
- Full FT variance: 0.0001-0.0014 (also stable)
- **Method differences >> seed noise** ‚úÖ

**Stability Advantage:**
- SST-2: LoRA is 2.0√ó more stable across seeds
- Overall: Task-dependent stability, not universal

### Interpretation

**Why Task-Dependent?**

1. **SST-2 (Sentiment) - Benefits from LoRA:**
   - Sentiment is well-represented in pretrained LLM knowledge
   - LoRA's low-rank constraint acts as implicit regularization
   - Prevents overfitting to task-specific quirks

2. **MRPC/RTE - No LoRA Advantage:**
   - Paraphrase/entailment require more task-specific adaptations
   - Base model representations less aligned with task requirements
   - Full fine-tuning's flexibility is beneficial

**Practical Implications:**
- ‚úÖ Use LoRA for tasks well-aligned with pretrained knowledge (sentiment, topic classification)
- ‚ö†Ô∏è Consider Full FT for tasks requiring significant representational shifts

---

## Research Question 2: Deployment Efficiency

### Methodology

**Benchmark Setup:**
- 20 deployment configurations tested
- 500 inference samples per configuration (warmup: 10)
- Metrics: latency (mean, p50, p95, p99), throughput, GPU/CPU memory

**Configurations:**
1. **Single LoRA Adapter** (9 configs: 3 tasks √ó 3 seeds)
2. **Full Fine-Tuned Models** (9 configs: 3 tasks √ó 3 seeds)
3. **Multi-Adapter (2 tasks)** (1 config: MRPC + SST-2)
4. **Multi-Adapter (3 tasks)** (1 config: MRPC + SST-2 + RTE)

### Results

#### 2.1 Main Finding: LoRA Has 32.9% Latency Penalty

| Metric | LoRA Adapter | Full Fine-Tuned | Difference |
|--------|--------------|-----------------|------------|
| **Mean Latency** | 34.58 ¬± 0.16 ms | 26.03 ¬± 0.46 ms | **+32.9%** |
| **Throughput** | 28.92 ¬± 0.13 req/s | 38.43 ¬± 0.67 req/s | **-24.7%** |
| **GPU Memory** | 1997 ¬± 8 MB | 1993 ¬± 8 MB | +0.2% |
| **Model Loading** | 1.28 ¬± 0.82 sec | 0.80 ¬± 0.04 sec | +60% |

**Statistical Significance:**
- Paired t-test: t = 43.40, **p < 0.000001** (highly significant)
- Effect size: Cohen's d = 24.6 (extremely large)

**Verdict:** LoRA is **significantly slower** for inference than Full Fine-Tuning.

#### 2.2 Multi-Adapter Overhead: Minimal

| Configuration | Mean Latency | Overhead vs Single LoRA |
|---------------|--------------|------------------------|
| Single LoRA | 34.58 ms | - |
| Multi-Adapter (2) | 35.54 ms | **+2.8%** |
| Multi-Adapter (3) | 34.80 ms | **+0.6%** |

**Key Insight:**  
Adapter swapping adds negligible overhead (<3%), making multi-task LoRA deployment efficient.

#### 2.3 Per-Task Breakdown

| Task | LoRA Latency | Full FT Latency | Overhead |
|------|--------------|-----------------|----------|
| MRPC | 34.43 ms | 26.10 ms | +31.9% |
| SST-2 | 34.61 ms | 26.11 ms | +32.6% |
| RTE | 34.71 ms | 25.88 ms | +34.1% |

**Consistency:** Latency penalty is uniform across all tasks (~33%).

### Interpretation

**Why is LoRA Slower?**

```
Full Fine-Tuning:
  Input ‚Üí Forward Pass (merged weights) ‚Üí Output
  
LoRA:
  Input ‚Üí Base Model Forward Pass
       ‚Üí Low-Rank Adapter (ŒîW = BA)
       ‚Üí Addition (W_base + ŒîW)
       ‚Üí Output
```

**LoRA adds computational steps:**
1. Base model forward pass
2. Adapter matrix multiplication (low-rank)
3. Addition to base weights

**Memory Usage:**
- Runtime memory is comparable (~2GB) because both load full model
- Storage savings: LoRA adapter ‚âà 4MB vs Full model ‚âà 2GB

### Practical Implications

**When to Use LoRA:**
‚úÖ Multi-task deployment (share base model, swap adapters)  
‚úÖ Storage-constrained environments (1000√ó smaller adapters)  
‚úÖ Frequent model updates (only update adapters)

**When to Use Full Fine-Tuning:**
‚úÖ Single-task deployment with latency requirements  
‚úÖ Maximum throughput needed  
‚úÖ Inference speed is critical

**Trade-Off Summary:**

| Factor | LoRA | Full FT | Winner |
|--------|------|---------|--------|
| Inference Latency | 34.6 ms | 26.0 ms | **Full FT** (-33%) |
| Throughput | 28.9 req/s | 38.4 req/s | **Full FT** (+33%) |
| Storage Size | 4 MB | 2000 MB | **LoRA** (-99.8%) |
| Multi-Task Serving | Efficient | Requires multiple models | **LoRA** |
| Memory (Runtime) | ~2 GB | ~2 GB | Tie |
| Training Time | Faster | Slower | **LoRA** |
| Training Memory | Lower | Higher | **LoRA** |

---

## Integrated Conclusions

### The LoRA Value Proposition

**LoRA is NOT universally better**, but offers specific advantages:

1. **Training Efficiency** ‚úÖ
   - 99.6% fewer trainable parameters (4K vs 1B)
   - Faster training, less GPU memory required

2. **Storage Efficiency** ‚úÖ
   - 500√ó smaller model files (4MB vs 2GB)
   - Critical for edge deployment or frequent updates

3. **Multi-Task Flexibility** ‚úÖ
   - Share one base model, swap adapters (<3% overhead)
   - Ideal for serving multiple tasks

4. **Task-Specific Stability** ‚úÖ‚ö†Ô∏è
   - Reduces drift on tasks aligned with pretrained knowledge (SST-2)
   - No universal advantage (MRPC, RTE)

**However, LoRA comes with costs:**

1. **Inference Latency** ‚ùå
   - 33% slower than Full FT
   - Lower throughput (25% reduction)

2. **Not Universal Drift Advantage** ‚ö†Ô∏è
   - Task-dependent benefits
   - Full FT can be better for tasks requiring large representational shifts

### Deployment Decision Framework

```
SINGLE-TASK DEPLOYMENT:
‚îú‚îÄ Latency-critical? 
‚îÇ  ‚îú‚îÄ Yes ‚Üí Full Fine-Tuning (33% faster)
‚îÇ  ‚îî‚îÄ No ‚Üí LoRA (storage benefits, easier updates)
‚îÇ
MULTI-TASK DEPLOYMENT:
‚îî‚îÄ Always ‚Üí LoRA (efficient adapter swapping, shared base model)

TASK CHARACTERISTICS:
‚îú‚îÄ Aligned with pretrained knowledge (sentiment, topics)?
‚îÇ  ‚îî‚îÄ LoRA (better stability, less drift)
‚îú‚îÄ Requires significant adaptation (paraphrase, entailment)?
‚îÇ  ‚îî‚îÄ Full FT (more flexibility)
```

### Research Contributions

1. **Empirical Evidence**: Quantified LoRA's trade-offs with rigorous benchmarking
2. **Task-Dependent Findings**: Challenged assumption of universal LoRA advantages
3. **Deployment Insights**: Real-world latency measurements for informed decisions
4. **Methodological Rigor**: Statistical testing, multiple seeds, negative controls

---

## Experimental Details

### Models and Training

**Base Model:** TinyLlama-1.1B-intermediate-step-1431k-3T
- Architecture: Llama-2 (11 layers, 2048 hidden dim)
- Pretrained on 3T tokens

**LoRA Configuration:**
- Rank: r=8 (optimal from hyperparameter search)
- Alpha: Task-specific (8-64, optimized per task)
- Target modules: q_proj, v_proj (attention query/value)
- Trainable params: 4,096 (0.4% of model)

**Full Fine-Tuning:**
- All 1.1B parameters trainable
- Learning rates: Task-optimized (3e-6 to 5e-6)

**Training Details:**
- 3 epochs, batch size 16 (effective)
- AdamW optimizer, cosine schedule
- Reproducible across 3 seeds (42, 1337, 2024)

### Tasks (GLUE Benchmark)

| Task | Type | Samples | Metric | Description |
|------|------|---------|--------|-------------|
| MRPC | Binary Classification | 408 | F1 | Paraphrase detection |
| SST-2 | Binary Classification | 500 | Accuracy | Sentiment analysis |
| RTE | Binary Classification | 277 | Accuracy | Textual entailment |

### Statistical Methods

**Significance Testing:**
- Paired t-tests (method √ó task √ó seed)
- Bonferroni correction for multiple comparisons
- Effect sizes (Cohen's d)

**Robustness Checks:**
- Negative controls (seed-to-seed variance)
- Multiple seeds (n=3) per configuration
- Variance decomposition (method vs seed effects)

---

## Limitations and Future Work

### Limitations

1. **Model Scale:**
   - Single model size (1.1B parameters)
   - Findings may vary for larger models (7B, 13B, 70B)

2. **Task Scope:**
   - Binary classification only
   - No generative tasks (summarization, translation)

3. **Deployment Environment:**
   - Single GPU benchmarking
   - No distributed serving or batching optimizations

4. **LoRA Configuration:**
   - Fixed rank (r=8)
   - Only q_proj/v_proj targets

### Future Work

1. **Scale Analysis:**
   - Test on larger models (Llama-2-7B, 13B)
   - Investigate if drift/latency patterns hold

2. **Task Diversity:**
   - Generative tasks (summarization, QA generation)
   - Multi-class classification
   - Regression tasks

3. **Advanced LoRA Variants:**
   - QLoRA (quantized LoRA)
   - AdaLoRA (adaptive rank)
   - IA¬≥ (Infused Adapter)

4. **Production Deployment:**
   - vLLM multi-tenant serving
   - Batching strategies
   - Mixed LoRA + Full FT workloads

5. **Theoretical Analysis:**
   - Why task-dependent drift reduction?
   - Mathematical characterization of LoRA's representational bias

---

## Reproducibility

All code, models, and data are available in the project repository:

**Key Files:**
- `research_question_1_representational_drift.ipynb` - RQ1 analysis
- `research_question_2_deployment_efficiency.ipynb` - RQ2 analysis
- `scripts/phase4/analyze_drift.py` - Drift computation
- `scripts/phase4b/deployment_benchmark.py` - Latency benchmarking
- `results/` - All experimental outputs

**Reproducing Results:**
1. Install dependencies: `pip install -r requirements.txt`
2. Run Phase 2 training: `python experiments/{lora,full}_finetune.py`
3. Extract representations: `python scripts/phase3/extract_all_representations.sh`
4. Analyze drift: `python scripts/phase4/analyze_drift.py`
5. Benchmark deployment: `python scripts/phase4b/deployment_benchmark.py`

**Computational Requirements:**
- GPU: NVIDIA L4 or equivalent (24GB VRAM)
- Time: ~8 hours for full pipeline
- Storage: ~50GB for all models and results

---

## References

1. Hu, E. J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." ICLR 2022.
2. Kornblith, S., et al. (2019). "Similarity of Neural Network Representations Revisited." ICML 2019.
3. Wang, A., et al. (2018). "GLUE: A Multi-Task Benchmark and Analysis Platform." EMNLP 2018.
4. Touvron, H., et al. (2023). "Llama 2: Open Foundation and Fine-Tuned Chat Models." arXiv:2307.09288.

---

## Conclusion

This comprehensive study demonstrates that **LoRA offers a nuanced trade-off** rather than universal advantages over Full Fine-Tuning:

‚úÖ **LoRA Wins:** Training efficiency, storage, multi-task deployment, task-specific stability  
‚ùå **Full FT Wins:** Inference speed, throughput, tasks requiring large adaptations

üéØ **Key Takeaway:** Choose your fine-tuning method based on your deployment scenario and task characteristics, not on blanket assumptions about parameter efficiency.

The decision between LoRA and Full Fine-Tuning should be **context-driven**, considering:
- Deployment architecture (single vs multi-task)
- Performance requirements (latency vs storage)
- Task alignment with pretrained knowledge

**Both methods have their place** in the modern NLP toolkit.

---

**Report Generated:** October 5, 2025  
**Project Status:** Complete ‚úÖ  
**Contact:** galavny@tau.ac.il
