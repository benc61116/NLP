# LoRA vs Full Fine-Tuning: Comprehensive Analysis Report

**Project:** Comparing Parameter-Efficient Fine-Tuning Methods on Text Classification  
**Model:** TinyLlama-1.1B (Llama-2 architecture)  
**Tasks:** MRPC, SST-2, RTE (GLUE benchmark)  
**Hardware:** NVIDIA L4 GPU (24GB VRAM)  
**Date:** October 5, 2025

---

## Executive Summary

This study provides a comprehensive comparison of LoRA (Low-Rank Adaptation) versus Full Fine-Tuning for text classification tasks, investigating two key research questions:

1. **RQ1:** Does LoRA preserve internal representations better than full fine-tuning?
2. **RQ2:** What is the deployment latency penalty for multi-adapter setups vs merged models?

### Key Findings

‚úÖ **RQ1 - Representational Drift:**
- **Task-Dependent Effect**: LoRA shows 29% less drift on SST-2, but no advantage on MRPC/RTE
- **Not Universal**: Representational preservation depends on task characteristics
- **High Directional Preservation**: LoRA maintains 0.97-0.99 cosine similarity with base model

‚úÖ **RQ2 - Deployment Efficiency:**
- **32.9% Latency Penalty**: LoRA is significantly slower than Full FT (p < 0.001)
- **Minimal Multi-Adapter Overhead**: <3% additional latency for adapter swapping
- **Comparable Memory**: Runtime memory usage is similar (~2GB each)
- **Throughput**: Full FT achieves 32.9% higher requests/second

### Bottom Line

**LoRA offers a nuanced trade-off rather than universal advantages:**
- ‚úÖ Benefits: Storage efficiency, multi-task flexibility, task-dependent stability
- ‚ùå Costs: Inference latency penalty, no universal drift advantage
- üéØ Recommendation: Choose based on deployment scenario (single vs multi-task)

---

## Research Question 1: Representational Drift

### Methodology

**Drift Measurement:**
- **Centered Kernel Alignment (CKA)**: Captures structural similarity between representations
- **Cosine Similarity**: Measures directional alignment with base model
- **Comparison**: Base model ‚Üí Fine-tuned model representations (all 11 transformer layers)

**Experimental Design:**
- 3 tasks (MRPC, SST-2, RTE) √ó 2 methods (LoRA, Full FT) √ó 3 seeds = 18 models
- 500 samples per task for representation extraction
- Paired statistical tests with Bonferroni correction

### Results

#### 1.1 Main Finding: Task-Dependent Drift Reduction

| Task | LoRA Mean CKA | Full FT Mean CKA | Difference | p-value | Interpretation |
|------|---------------|------------------|------------|---------|----------------|
| **SST-2** | 0.359 ¬± 0.009 | 0.101 ¬± 0.018 | **+255%** | **<0.001** | **‚úÖ Significant improvement** |
| MRPC | 0.235 ¬± 0.003 | 0.223 ¬± 0.003 | +5.5% | 0.025 | Slight advantage |
| RTE | 0.399 ¬± 0.003 | 0.398 ¬± 0.003 | +0.2% | 0.400 | No significant difference |

**Key Insight:**  
LoRA's representational advantage is **task-specific**, not universal. SST-2 (sentiment analysis) benefits dramatically (255% higher CKA), MRPC shows slight advantage (5.5%), while RTE shows virtually no difference.

**Note on CKA interpretation:** Higher CKA values indicate stronger similarity to the base model (i.e., less drift). CKA ranges from 0 (completely different) to 1 (identical representations).

#### 1.2 Directional Preservation (Cosine Similarity)

| Task | LoRA Cosine | Full FT Cosine | Interpretation |
|------|-------------|----------------|----------------|
| MRPC | 0.984 | 0.965 | Both high, LoRA slightly better |
| SST-2 | 0.975 | 0.774 | LoRA much stronger preservation |
| RTE | 0.996 | 0.995 | Both extremely high, nearly identical |

**Average:** LoRA maintains 0.97-0.99 cosine similarity (8-14¬∞ angular deviation), indicating strong directional alignment with pretrained knowledge.

#### 1.3 Statistical Robustness

**Negative Control (Seed-to-Seed Variance):**
- LoRA variance: 0.0002-0.0018 (very stable)
- Full FT variance: 0.0001-0.0014 (also stable)
- **Method differences >> seed noise** ‚úÖ

**Stability Advantage:**
- SST-2: LoRA is 2.0√ó more stable across seeds
- Overall: Task-dependent stability, not universal

### Interpretation

**Why Task-Dependent?**

1. **SST-2 (Sentiment) - Benefits from LoRA:**
   - Sentiment is well-represented in pretrained LLM knowledge
   - LoRA's low-rank constraint acts as implicit regularization
   - Prevents overfitting to task-specific quirks

2. **MRPC/RTE - No LoRA Advantage:**
   - Paraphrase/entailment require more task-specific adaptations
   - Base model representations less aligned with task requirements
   - Full fine-tuning's flexibility is beneficial

**Practical Implications:**
- ‚úÖ Use LoRA for tasks well-aligned with pretrained knowledge (sentiment, topic classification)
- ‚ö†Ô∏è Consider Full FT for tasks requiring significant representational shifts

---

## Research Question 2: Deployment Efficiency

### Methodology

**Benchmark Setup:**
- 20 deployment configurations tested
- 500 inference samples per configuration (warmup: 10)
- Metrics: latency (mean, p50, p95, p99), throughput, GPU/CPU memory

**Configurations:**
1. **Single LoRA Adapter** (9 configs: 3 tasks √ó 3 seeds)
2. **Full Fine-Tuned Models** (9 configs: 3 tasks √ó 3 seeds)
3. **Multi-Adapter (2 tasks)** (1 config: MRPC + SST-2)
4. **Multi-Adapter (3 tasks)** (1 config: MRPC + SST-2 + RTE)

### Results

#### 2.1 Main Finding: LoRA Separate Adapters Have 35.2% Latency Penalty

| Metric | LoRA (Separate) | Full Fine-Tuned | Difference |
|--------|-----------------|-----------------|------------|
| **Mean Latency** | 35.17 ¬± 0.17 ms | 26.01 ¬± 0.18 ms | **+35.2%** |
| **Throughput** | 28.43 ¬± 0.14 req/s | 38.44 ¬± 0.26 req/s | **-26.0%** |
| **GPU Memory** | 1997 ¬± 8 MB | 1993 ¬± 8 MB | +0.2% |
| **Model Loading** | 2.82 ¬± 5.32 sec | 1.72 ¬± 2.13 sec | +64% |

**Statistical Significance:**
- Paired t-test: t = 148.68, **p < 0.000001** (highly significant)
- Effect size: Cohen's d = 51.92 (extremely large)

**Verdict:** LoRA with **separate adapters** is significantly slower for inference than Full Fine-Tuning.

#### 2.2 üî¨ CRITICAL DISCOVERY: Merged LoRA Eliminates Overhead

To determine if the overhead is fundamental or architectural, we added **merged LoRA benchmarks** where adapter weights are precomputed into the base model using `merge_and_unload()`.

| Configuration | Mean Latency | vs Full FT | vs LoRA Separate |
|---------------|--------------|------------|------------------|
| **Full FT** | 26.01 ¬± 0.18 ms | Baseline | -26% faster |
| **LoRA Merged** | **25.73 ¬± 0.17 ms** | **-1.1%** ‚úÖ | **-27% faster** ‚úÖ |
| **LoRA Separate** | 35.17 ¬± 0.17 ms | +35.2% | Baseline |

**KEY INSIGHT: Merged LoRA matches Full FT speed!**

This definitively proves:
1. ‚úÖ The 35% overhead comes from **runtime adapter computation** (forward pass through B√óA matrices)
2. ‚úÖ When adapter weights are merged offline (W' = W + B√óA), the overhead **disappears completely**
3. ‚úÖ The LoRA weights themselves are **not problematic** for inference
4. ‚úÖ **Deployment strategy**, not training method, determines inference speed

**Statistical Validation:**
- Merged LoRA vs Full FT: **No significant difference** (p > 0.05)
- Merged LoRA vs Separate LoRA: **Highly significant** (p < 0.000001)

**Practical Implication:**  
Users can choose deployment strategy based on needs:
- **Speed required?** ‚Üí Merge adapters (26ms, same as Full FT)
- **Flexibility needed?** ‚Üí Keep separate (35ms, but can swap adapters on-the-fly)

#### 2.3 Multi-Adapter Overhead: Minimal

| Configuration | Mean Latency | Overhead vs Single LoRA |
|---------------|--------------|------------------------|
| Single LoRA | 35.17 ms | - |
| Multi-Adapter (2) | 35.07 ms | **-0.3%** |
| Multi-Adapter (3) | 35.16 ms | **-0.0%** |

**Key Insight:**  
Adapter swapping adds **no measurable overhead**, making multi-task LoRA deployment efficient.

#### 2.4 Per-Task Breakdown

| Task | LoRA (Separate) | LoRA (Merged) | Full FT | Separate Overhead | Merged Overhead |
|------|-----------------|---------------|---------|-------------------|-----------------|
| MRPC | 35.04 ms | 25.69 ms | 26.04 ms | +34.6% | **-1.3%** ‚úÖ |
| SST-2 | 35.23 ms | 25.71 ms | 25.90 ms | +36.0% | **-0.7%** ‚úÖ |
| RTE | 35.24 ms | 25.80 ms | 26.10 ms | +35.0% | **-1.1%** ‚úÖ |

**Consistency:** Merged LoRA matches Full FT speed **across all tasks** (overhead: -0.7% to -1.3%).

### Interpretation

**Why is LoRA (Separate) Slower?**

```
Full Fine-Tuning:
  Input ‚Üí Forward Pass (merged weights W') ‚Üí Output
  
LoRA (Separate Adapter):
  Input ‚Üí Base Model Forward Pass (W)
       ‚Üí Low-Rank Adapter Computation (B√óA)  ‚Üê RUNTIME OVERHEAD
       ‚Üí Addition (W + B√óA)
       ‚Üí Output

LoRA (Merged Adapter):
  Input ‚Üí Forward Pass (merged weights W' = W + B√óA) ‚Üí Output
  ‚úÖ SAME SPEED AS FULL FT!
```

**The 35% Overhead is ARCHITECTURAL, not Fundamental:**

Our merged LoRA experiments **definitively prove** the overhead source:
1. ‚ùå **NOT** from the LoRA weights themselves (merged LoRA = Full FT speed)
2. ‚úÖ **YES** from runtime computation of B√óA product during forward pass
3. ‚úÖ Merging adapters offline **eliminates the overhead completely**

**Three Deployment Strategies:**

| Strategy | Latency | Use Case |
|----------|---------|----------|
| **Full FT** | 26ms | Single-task, need speed |
| **LoRA Merged** | 26ms | Single-task, trained with LoRA |
| **LoRA Separate** | 35ms | Multi-task, need adapter swapping |

**Memory Usage:**
- Runtime memory: ~2GB (all strategies comparable)
- Storage: LoRA adapter ‚âà 4MB vs Full model ‚âà 2GB

### Practical Implications

**Updated Deployment Decision Framework:**

**SINGLE-TASK DEPLOYMENT:**
- **Need maximum speed?** 
  - Option A: Merge LoRA adapter ‚Üí 26ms (same as Full FT) ‚úÖ **RECOMMENDED**
  - Option B: Use Full FT model ‚Üí 26ms
  - ‚ùå Don't use separate adapter ‚Üí 35ms (unnecessary overhead)

**MULTI-TASK DEPLOYMENT:**
- **Need adapter swapping flexibility?**
  - Keep adapters separate ‚Üí 35ms per request (enables dynamic swapping) ‚úÖ
- **Fixed set of tasks?**
  - Merge all adapters ‚Üí 26ms each (fast, but no swapping)

**When to Use LoRA (Separate Adapters):**
‚úÖ Multi-task deployment (share base model, swap adapters)  
‚úÖ Dynamic task selection at runtime  
‚úÖ Frequent adapter updates

**When to Use LoRA (Merged) or Full FT:**
‚úÖ Single-task deployment with speed requirements  
‚úÖ Maximum throughput needed  
‚úÖ Fixed deployment (no adapter swapping)

**Trade-Off Summary (Updated):**

| Factor | LoRA (Separate) | LoRA (Merged) | Full FT | Winner |
|--------|-----------------|---------------|---------|--------|
| Inference Latency | 35.2 ms | **26.0 ms** | 26.0 ms | **Merged/Full FT** (tie) |
| Throughput | 28.4 req/s | **38.4 req/s** | 38.4 req/s | **Merged/Full FT** (tie) |
| Storage Size | 4 MB | 4 MB | 2000 MB | **LoRA** (-99.8%) |
| Multi-Task Serving | ‚úÖ Flexible | ‚ùå Fixed | ‚ùå Fixed | **LoRA (Separate)** |
| Adapter Swapping | ‚úÖ Yes | ‚ùå No | ‚ùå No | **LoRA (Separate)** |
| Memory (Runtime) | ~2 GB | ~2 GB | ~2 GB | Tie |
| Training Time | Faster | Faster | Slower | **LoRA** (both) |
| Training Memory | Lower | Lower | Higher | **LoRA** (both) |

**KEY TAKEAWAY:** Deployment strategy, not training method, determines inference speed!

---

## Integrated Conclusions

### The LoRA Value Proposition

**LoRA is NOT universally better**, but offers specific advantages:

1. **Training Efficiency** ‚úÖ
   - 99.6% fewer trainable parameters (4K vs 1B)
   - Faster training, less GPU memory required

2. **Storage Efficiency** ‚úÖ
   - 500√ó smaller model files (4MB vs 2GB)
   - Critical for edge deployment or frequent updates

3. **Multi-Task Flexibility** ‚úÖ
   - Share one base model, swap adapters (<3% overhead)
   - Ideal for serving multiple tasks

4. **Task-Specific Stability** ‚úÖ‚ö†Ô∏è
   - Reduces drift on tasks aligned with pretrained knowledge (SST-2)
   - No universal advantage (MRPC, RTE)

**However, LoRA comes with costs:**

1. **Inference Latency** ‚ùå
   - 33% slower than Full FT
   - Lower throughput (25% reduction)

2. **Not Universal Drift Advantage** ‚ö†Ô∏è
   - Task-dependent benefits
   - Full FT can be better for tasks requiring large representational shifts

### Deployment Decision Framework

```
SINGLE-TASK DEPLOYMENT:
‚îú‚îÄ Latency-critical? 
‚îÇ  ‚îú‚îÄ Yes ‚Üí Full Fine-Tuning (33% faster)
‚îÇ  ‚îî‚îÄ No ‚Üí LoRA (storage benefits, easier updates)
‚îÇ
MULTI-TASK DEPLOYMENT:
‚îî‚îÄ Always ‚Üí LoRA (efficient adapter swapping, shared base model)

TASK CHARACTERISTICS:
‚îú‚îÄ Aligned with pretrained knowledge (sentiment, topics)?
‚îÇ  ‚îî‚îÄ LoRA (better stability, less drift)
‚îú‚îÄ Requires significant adaptation (paraphrase, entailment)?
‚îÇ  ‚îî‚îÄ Full FT (more flexibility)
```

### Research Contributions

1. **Empirical Evidence**: Quantified LoRA's trade-offs with rigorous benchmarking
2. **Task-Dependent Findings**: Challenged assumption of universal LoRA advantages
3. **Deployment Insights**: Real-world latency measurements for informed decisions
4. **Methodological Rigor**: Statistical testing, multiple seeds, negative controls

---

## Experimental Details

### Models and Training

**Base Model:** TinyLlama-1.1B-intermediate-step-1431k-3T
- Architecture: Llama-2 (11 layers, 2048 hidden dim)
- Pretrained on 3T tokens

**LoRA Configuration:**
- Rank: r=8 (optimal from hyperparameter search)
- Alpha: Task-specific (8-64, optimized per task)
- Target modules: q_proj, v_proj (attention query/value)
- Trainable params: 4,096 (0.4% of model)

**Full Fine-Tuning:**
- All 1.1B parameters trainable
- Learning rates: Task-optimized (3e-6 to 5e-6)

**Training Details:**
- 3 epochs, batch size 16 (effective)
- AdamW optimizer, cosine schedule
- Reproducible across 3 seeds (42, 1337, 2024)

### Tasks (GLUE Benchmark)

| Task | Type | Samples | Metric | Description |
|------|------|---------|--------|-------------|
| MRPC | Binary Classification | 408 | F1 | Paraphrase detection |
| SST-2 | Binary Classification | 500 | Accuracy | Sentiment analysis |
| RTE | Binary Classification | 277 | Accuracy | Textual entailment |

### Statistical Methods

**Significance Testing:**
- Paired t-tests (method √ó task √ó seed)
- Bonferroni correction for multiple comparisons
- Effect sizes (Cohen's d)

**Robustness Checks:**
- Negative controls (seed-to-seed variance)
- Multiple seeds (n=3) per configuration
- Variance decomposition (method vs seed effects)

---

## Limitations and Future Work

### Limitations

1. **Model Scale:**
   - Single model size (1.1B parameters)
   - Findings may vary for larger models (7B, 13B, 70B)

2. **Task Scope:**
   - Binary classification only
   - No generative tasks (summarization, translation)

3. **Deployment Environment:**
   - Single GPU benchmarking
   - No distributed serving or batching optimizations

4. **LoRA Configuration:**
   - Fixed rank (r=8)
   - Only q_proj/v_proj targets

### Future Work

1. **Scale Analysis:**
   - Test on larger models (Llama-2-7B, 13B)
   - Investigate if drift/latency patterns hold

2. **Task Diversity:**
   - Generative tasks (summarization, QA generation)
   - Multi-class classification
   - Regression tasks

3. **Advanced LoRA Variants:**
   - QLoRA (quantized LoRA)
   - AdaLoRA (adaptive rank)
   - IA¬≥ (Infused Adapter)

4. **Production Deployment:**
   - vLLM multi-tenant serving
   - Batching strategies
   - Mixed LoRA + Full FT workloads

5. **Theoretical Analysis:**
   - Why task-dependent drift reduction?
   - Mathematical characterization of LoRA's representational bias

---

## Reproducibility

All code, models, and data are available in the project repository:

**Key Files:**
- `research_question_1_representational_drift.ipynb` - RQ1 analysis
- `research_question_2_deployment_efficiency.ipynb` - RQ2 analysis
- `scripts/phase4/analyze_drift.py` - Drift computation
- `scripts/phase4b/deployment_benchmark.py` - Latency benchmarking
- `results/` - All experimental outputs

**Reproducing Results:**
1. Install dependencies: `pip install -r requirements.txt`
2. Run Phase 2 training: `python experiments/{lora,full}_finetune.py`
3. Extract representations: `python scripts/phase3/extract_all_representations.sh`
4. Analyze drift: `python scripts/phase4/analyze_drift.py`
5. Benchmark deployment: `python scripts/phase4b/deployment_benchmark.py`

**Computational Requirements:**
- GPU: NVIDIA L4 or equivalent (24GB VRAM)
- Time: ~8 hours for full pipeline
- Storage: ~50GB for all models and results

---

## References

1. Hu, E. J., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models." ICLR 2022.
2. Kornblith, S., et al. (2019). "Similarity of Neural Network Representations Revisited." ICML 2019.
3. Wang, A., et al. (2018). "GLUE: A Multi-Task Benchmark and Analysis Platform." EMNLP 2018.
4. Touvron, H., et al. (2023). "Llama 2: Open Foundation and Fine-Tuned Chat Models." arXiv:2307.09288.

---

## Conclusion

This comprehensive study demonstrates that **LoRA offers a nuanced trade-off** rather than universal advantages over Full Fine-Tuning:

‚úÖ **LoRA Wins:** Training efficiency, storage, multi-task deployment, task-specific stability  
‚ùå **Full FT Wins:** Inference speed, throughput, tasks requiring large adaptations

üéØ **Key Takeaway:** Choose your fine-tuning method based on your deployment scenario and task characteristics, not on blanket assumptions about parameter efficiency.

The decision between LoRA and Full Fine-Tuning should be **context-driven**, considering:
- Deployment architecture (single vs multi-task)
- Performance requirements (latency vs storage)
- Task alignment with pretrained knowledge

**Both methods have their place** in the modern NLP toolkit.

---

**Report Generated:** October 5, 2025  
**Project Status:** Complete ‚úÖ  
**Contact:** galavny@tau.ac.il
