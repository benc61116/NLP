You are an expert NLP research engineer and strategic project planner. Your task is to create a comprehensive, step-by-step implementation plan for the research project "LoRA Under the Microscope: Drift, Deployment and Sparsity."

<persistence>
- You are an agent - please keep going until the plan is completely developed, before ending your turn.
- Only terminate when you have created a thorough, actionable plan that addresses all project requirements and grading criteria.
- Never stop at uncertainty — research or deduce the most reasonable approach based on research best practices and continue.
- Do not ask for clarification on implementation details — make informed assumptions and document them clearly.
</persistence>

<tool_preambles>
- Begin by clearly restating the research objectives and their significance within NLP research.
- Outline your structured approach for breaking down this complex project into manageable steps optimized for 3-VM parallel execution.
- As you develop each step, explain the scientific rationale and experimental design principles.
- Conclude with a summary of the complete implementation roadmap that ensures rigorous methodology and reproducible results.
</tool_preambles>

<prompt_formatting>
CRITICAL: For every step in the plan, you MUST provide the agent prompt in a proper code block using triple backticks (```). This is essential for the human to copy and use these prompts directly. Format each agent prompt like this:

```
[Complete agent prompt with full context and instructions]
```

Never provide prompts as plain text - they must always be in code blocks for easy copying.
</prompt_formatting>

## PROJECT CONTEXT

**CORE RESEARCH FOCUS - DO NOT DRIFT FROM THESE:**
- **Primary Question 1**: Representational drift comparison - Does LoRA truly preserve model internal representations better than full fine-tuning using centered-kernel alignment (CKA) and layer-wise cosine similarity?
- **Primary Question 2**: Deployment efficiency trade-offs - What is the real-world latency penalty when deploying multiple LoRA adapters side-by-side versus merging them in vLLM?
- **Target Model**: Llama-2-1.3B (specified for computational feasibility)
- **Evaluation Tasks**: GLUE-MRPC (sentence-pair classification) and SQuAD v2 (question-answering)
- **Research Hypotheses to Test**: We hypothesize that LoRA will achieve ≤3% accuracy drop AND either ≥20% less representational drift OR ≤30% inference overhead. Both confirming and refuting these hypotheses constitute valid scientific findings.

**Research Significance:**
These questions address critical gaps in efficient fine-tuning research with direct implications for continual learning, catastrophic forgetting understanding, and production deployment decisions for multi-task systems.

**Technical Specifications:**
- **Training Approaches**: (a) Full fine-tuning vs (b) LoRA with rank 8
- **Drift Metrics**: Centered-kernel alignment (CKA) and layer-wise cosine similarity on 1000 validation examples across all transformer layers
- **Deployment Metrics**: vLLM inference engine measuring tokens/second, 95th-percentile latency, GPU memory usage at batch sizes 1-16

**Infrastructure Constraints:**
- **Available Resources**: Exactly 3 GPU VMs for parallel execution
- **Experiment Tracking**: Weights & Biases (wandb)
  - Project: "NLP"
  - Entity: "galavny-tel-aviv-university"
  - Dashboard: https://wandb.ai/galavny-tel-aviv-university/NLP
  - Environment: `WANDB_PROJECT=NLP`, `WANDB_ENTITY=galavny-tel-aviv-university`
- **Required Technical Stack**: PEFT library for LoRA implementation, vLLM for deployment benchmarking

**AGENT AUTONOMY**: While maintaining scientific rigor and staying focused on the core research questions, you have full creative freedom in:
- Implementation approaches and code architecture decisions
- Additional analyses that support or extend the core research questions
- Optimization strategies and efficiency improvements for the 3-VM setup  
- Supplementary experiments that strengthen conclusions (within scope)
- Creative solutions to technical challenges and experimental design innovations

<context_gathering>
Goal: Create a scientifically rigorous plan that maximizes 3-VM parallel utilization while adhering to research methodology best practices.

Method:
- Design experiments following gold-standard practices: proper train/validation/test splits, baseline comparisons, statistical significance testing
- Determine optimal task distribution across exactly 3 VMs based on computational requirements and dependencies
- Include comprehensive sanity checks (overfitting small samples) and reproducibility safeguards
- Plan robust experimental design with appropriate controls and multiple random seeds

Critical Requirements:
- All experiments must be reproducible with detailed environment specifications
- Include meaningful baseline comparisons (naive classifiers, current SOTA methods)
- Account for all sources of randomness (seeds, prompt effects, decoding schemes)
- Implement proper statistical validation across multiple runs
</context_gathering>

## IMPLEMENTATION CRITERIA ALIGNMENT

**Research Question Quality:**
- Ensure the plan addresses clear, original, and significant research questions relevant to parameter-efficient fine-tuning
- Questions must be concise, well-motivated, and positioned within current NLP research gaps

**Ambitiousness and Effort:**
- Design methodology that demonstrates intellectual risk-taking and innovation
- Push boundaries while maintaining scientific rigor
- Include novel analysis approaches (e.g., layer-wise drift analysis, comprehensive deployment benchmarking)

**Methodology:**
- **Experimental Design**: Proper train/val/test splits, no data leakage, appropriate model selection
- **Baseline Comparisons**: Include naive majority vote classifier and current literature baselines
- **Hyperparameter Selection**: Sensible, well-justified hyperparameter choices with systematic tuning
- **Randomness Control**: Multiple random seeds, statistical significance testing, confidence intervals
- **Sanity Checks**: Verify model can overfit small data samples before full training
- **Reproducibility**: Exact environment specifications, code availability, deterministic execution
- **Data Collection & Processing**: Proper dataset handling, preprocessing pipelines, and validation procedures
- **Statistical Analysis**: Implement significance testing, effect size calculations, and confidence intervals

<code_editing_rules>
<guiding_principles>
- Scientific Rigor: Every experiment must follow gold-standard methodology with proper controls
- Statistical Validity: Include multiple runs, significance testing, and confidence intervals
- Reproducibility: Detailed environment specs, deterministic execution, accessible code/data
- Efficiency: Maximize 3-VM parallel utilization while maintaining experimental integrity
</guiding_principles>

<experimental_standards>
- Use PyTorch 2.x with proper distributed training setup across 3 VMs
- Implement comprehensive logging for debugging and reproducibility
- Include automatic sanity checks (small-sample overfitting) before full experiments
- Use wandb sweeps for systematic hyperparameter optimization
- Implement proper model checkpointing with automatic resumption on failures
- Design statistical analysis pipelines for significance testing and effect size calculation
</experimental_standards>
</code_editing_rules>

## CRITICAL IMPLEMENTATION CONSIDERATIONS

**3-VM Distributed Architecture:**
- You must determine the optimal distribution of tasks across exactly 3 VMs based on computational requirements, dependencies, and load balancing
- Consider task dependencies and design appropriate synchronization points
- Implement fault tolerance and dynamic load balancing strategies
- Justify your VM allocation decisions based on resource utilization and experimental efficiency
- **CRITICAL**: Explain how to coordinate experiments across 3 VMs using wandb and git from both perspectives:
  - **Coding perspective**: How to structure code for distributed execution, shared configurations, result aggregation
  - **Technical/operational perspective**: Actual commands and procedures for running experiments across VMs, synchronization protocols, file sharing

**MANDATORY EXPERIMENTAL RIGOR (Critical for Methodology Grading):**
- **Data Integrity & Splits**: MUST follow best practices to avoid data leakage and overfitting with proper train/validation/test splits, stratification, and balanced splits
- **Appropriate Models & Hyperparameters**: Use sensible, well-justified hyperparameter choices with systematic approaches
- **REQUIRED Baseline Comparisons**: Include ALL of the following baselines:
  - Naive majority vote classifier 
  - Random baseline
  - Current literature approaches (SOTA methods)
  - This is the only way to frame results meaningfully
- **Randomness Control**: Account for ALL sources of randomness:
  - Multiple random seeds (minimum 3, more if computationally feasible)
  - Prompt effects and variations
  - Decoding scheme randomness
  - Statistical significance testing with confidence intervals
- **CRITICAL Sanity Checks**: Verify models can overfit small samples (10-100 examples) before full training - this catches bugs and validates implementation
- **Reproducibility Requirements**: Provide exact experimental settings, environment specifications, code access, and detailed documentation for full reproducibility

**Additional Quality Assurance:**
- **Hardware Documentation**: GPU types, memory specifications, driver versions
- **Data Provenance**: Dataset versions, preprocessing steps, filtering criteria  
- **Error Handling**: Detailed protocols for handling experimental failures and edge cases

## DELIVERABLE SPECIFICATION

Create a `plan.md` file with the following structure optimized for scientific rigor:

```markdown
# LoRA Research Implementation Plan

## Research Questions & Significance
[Clear problem statement with scientific motivation]

## 3-VM Resource Allocation Strategy
[Your optimal distribution of tasks across 3 VMs with justification for allocation decisions and load balancing approach]

## Distributed Coordination Protocol
[Detailed explanation of how to coordinate experiments across 3 VMs using wandb and git, including both code structure and operational procedures]

## Step 1: Environment Setup & Sanity Checks
### Agent Prompt
```
[Complete setup instructions with reproducibility requirements - MUST be in code block]
```
### 3-VM Distribution
[How you distribute setup tasks across VMs with rationale]
### Sanity Check Protocol
[Small-sample overfitting verification procedures]
### Validation Criteria
[Environment validation and baseline functionality tests to ensure experimental setup is working correctly]

## Step 2: Baseline Establishment
### Agent Prompt
```
[Baseline model training with proper experimental controls - MUST be in code block]
```
### Statistical Design
[Multiple seeds, significance testing, effect size calculation]
### Baseline Comparisons
[Naive classifiers and literature SOTA implementations]

[Continue for all experimental steps...]

## Statistical Analysis Pipeline
[Comprehensive significance testing and effect size analysis]

## Reproducibility Checklist
[Complete environment specification and validation procedures]
```

**MANDATORY FORMATTING REQUIREMENTS:**
- Every "Agent Prompt" section MUST contain the prompt in triple backticks (```)
- These code blocks must be copy-pasteable for direct use with specialized agents
- Include complete context and background in each prompt since agents work independently
- Each prompt should be self-contained with all necessary project context

**Final Instructions:**
- **NON-NEGOTIABLE**: Follow the mandatory experimental rigor requirements above - these are critical for methodology grading
- **CRITICAL**: Ensure every step follows gold-standard experimental methodology with proper controls, baselines, and statistical validation
- Design for maximum efficiency across exactly 3 GPU VMs - determine optimal task distribution and justify your allocation strategy  
- **MANDATORY**: Implement ALL required baselines (majority vote, random, SOTA) and sanity checks (small sample overfitting)
- **ESSENTIAL**: Account for all sources of randomness and provide comprehensive reproducibility documentation
- Create detailed protocols for handling experimental failures and edge cases
- Focus exclusively on research methodology and getting results - exclude paper writing considerations
- **CRITICAL**: Format ALL agent prompts in proper code blocks using triple backticks

Generate a meticulous, scientifically rigorous plan that adheres to the highest experimental standards while optimally distributing work across 3 VMs and maintaining focus on the core research questions.