# LoRA Under the Microscope: Representation Drift and Deployment Trade-offs

**Paper:** "LoRA Under the Microscope: Representation Drift and Deployment Trade-offs for Text Classification"  
**Authors:** Gal Avny, Ben Cohen  
**Model:** TinyLlama-1.1B  
**Tasks:** MRPC, SST-2, RTE (GLUE classification)

This repository contains code to reproduce all experiments and results from our ACL 2024 submission on comparing LoRA (Low-Rank Adaptation) vs Full Fine-Tuning for text classification.

---

## Key Findings

1. **RQ1 (Representational Drift):** LoRA preserves base model representations better on SST-2 (28.7% drift reduction, p<0.001) but not on MRPC or RTE - preservation is task-dependent
2. **RQ2 (Deployment Efficiency):** Merged LoRA adapters match full fine-tuning speed (25.5ms vs 25.5ms); separate adapters add 37.5% latency overhead
3. **Training Stability:** LoRA prevents catastrophic failures on small datasets (RTE: +22.6pp, no failures vs full FT)

See `ANALYSIS_REPORT.md` for complete findings.

---

## What's In This Repository

### Available in Git (Ready to Use)
- **Final Results:**
  - `ANALYSIS_REPORT.md` - Complete analysis report
  - `drift_analysis/` - RQ1 results (drift metrics, performance data)
  - `deployment/` - RQ2 results (latency benchmarks)
  - `research_question_*.ipynb` - Analysis notebooks
  
- **Optimal Hyperparameters** (Phase 1 outputs):
  - `analysis/mrpc_full_finetune_optimal.yaml`
  - `analysis/mrpc_lora_optimal.yaml`
  - `analysis/sst2_full_finetune_optimal.yaml`
  - `analysis/sst2_lora_optimal.yaml`
  - `analysis/rte_full_finetune_optimal.yaml`
  - `analysis/rte_lora_optimal.yaml`

- **All Code:**
  - `experiments/` - Training scripts (LoRA, Full FT, Optuna)
  - `scripts/` - Reproduction scripts organized by phase
  - `shared/` - Utilities (data loading, metrics, configs)

### Large Files (Need to Generate)
To fully reproduce the experiments, you will need to generate:
- **18 trained models** (~36GB total): 3 tasks × 2 methods × 3 seeds
- **Base model representations** (~48GB): Extracted in Phase 0/3
- **Fine-tuned representations** (~12GB): Extracted in Phase 3

All of these are generated by running the reproduction steps below.

---

## Quick Start (View Results Only)

If you just want to see the results without reproducing experiments:

```bash
# 1. Clone repository
git clone https://github.com/benc61116/NLP.git
cd NLP

# 2. Install minimal dependencies
pip install pandas matplotlib seaborn jupyter

# 3. View results
cat ANALYSIS_REPORT.md                    # Read final report
jupyter notebook                          # Open analysis notebooks
```

All analysis results are already in git (`drift_analysis/`, `deployment/`).

---

## Full Reproduction (Step-by-Step)

To reproduce all experiments from scratch, follow these phases in order.

### Environment Setup

```bash
# Python 3.10+ required
conda create -n lora-reproduction python=3.10
conda activate lora-reproduction

# Install all dependencies
pip install -r requirements.txt

# Verify installation
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import peft; print(f'PEFT: {peft.__version__}')"
```

**Weights & Biases (WandB) Setup:**

WandB is used for experiment tracking and logging during training (metrics, hyperparameters, training curves). It is **not** used for downloading pre-trained models.

1. Create a free account at https://wandb.ai
2. Get your API key from https://wandb.ai/authorize
3. Configure WandB:

```bash
# Login to WandB (enter API key when prompted)
wandb login

# Set your WandB entity (username or team name)
export WANDB_ENTITY=your-username-or-team

# Optional: Set custom project names for different phases
export WANDB_PROJECT=NLP  # Default project name
```

**Important:** Update `shared/config.yaml` with your WandB entity:
```yaml
wandb:
  entity: "your-username-or-team"  # Change this to your WandB username/team
```

If you don't want to use WandB, you can disable it:
```bash
export WANDB_MODE=disabled
```

**Hardware Requirements:**
- NVIDIA GPU with 24GB VRAM (L4, RTX 3090, RTX 4090, A5000, etc.)
- ~100GB free disk space
- Ubuntu 20.04+ with CUDA 11.8+

---

### Phase 0: Dataset Download & Sanity Checks (Configs in Git)

**Purpose:** Download datasets and validate implementation.

```bash
# Download MRPC, SST-2, RTE from HuggingFace
python scripts/download_datasets.py

# Expected output:
# data/
#   ├── mrpc/     (3,668 train samples)
#   ├── sst2/     (67,349 train samples)
#   └── rte/      (2,490 train samples)

# Run sanity checks (optional - validates implementation)
cd scripts/phase0
bash vm1.sh  # Tests overfitting on 10 samples per task
```

**Phase 0 Complete:** Datasets downloaded, implementation validated.

---

### Phase 1: Hyperparameter Optimization (Results in Git)

**Purpose:** Find optimal hyperparameters for each task/method combination.

**Status:** Already complete - optimal configs are in `analysis/*.yaml`

**If you want to re-run HPO (optional, ~4 hours):**

```bash
cd scripts/phase1
bash vm1.sh  # Runs Optuna (15 trials × 2 methods × 3 tasks = 90 trials)
```

**Outputs (already in git):**
- `analysis/mrpc_full_finetune_optimal.yaml`
- `analysis/mrpc_lora_optimal.yaml`
- `analysis/sst2_full_finetune_optimal.yaml`
- `analysis/sst2_lora_optimal.yaml`
- `analysis/rte_full_finetune_optimal.yaml`
- `analysis/rte_lora_optimal.yaml`

---

### Phase 2: Train Production Models (Need to Reproduce)

**Purpose:** Train 18 final models (3 tasks × 2 methods × 3 seeds).

```bash
cd scripts/phase2

# Train all models (Full FT + LoRA for all tasks)
bash vm2.sh  # Trains both methods × 3 tasks × 3 seeds (~20-30 hours)

# Or train separately:
# Full FT only: Edit vm2.sh to comment out LoRA section
# LoRA only: bash rerun_lora_only.sh
```

**Expected outputs in `results/`:**
```
results/
├── full_finetune_model_mrpc_seed42/
├── full_finetune_model_mrpc_seed1337/
├── full_finetune_model_mrpc_seed2024/
├── full_finetune_model_sst2_seed42/
├── ... (9 full FT models)
├── lora_adapter_mrpc_seed42/
├── lora_adapter_mrpc_seed1337/
├── ... (9 LoRA adapters)
```

---

### Phase 3: Extract Representations (Need to Reproduce)

**Purpose:** Extract layer-wise representations for drift analysis.

**Step 1: Extract Base Model Representations**

These are representations from the **untrained** TinyLlama model (run once before training):

```bash
# Extract base representations for all tasks (~2 hours, ~48GB output)
python scripts/extract_base_representations.py

# Expected output:
# base_representations/
#   ├── mrpc_base_representations/
#   ├── sst2_base_representations/
#   └── rte_base_representations/
```

**Step 2: Extract Fine-tuned Model Representations**

Extract representations from the 18 trained models:

```bash
cd scripts/phase3

# Extract from all 18 models (~8 hours)
bash vm1.sh

# Expected output:
# results/phase3_representations/representations/
#   ├── full_finetune_mrpc_seed42/
#   ├── full_finetune_mrpc_seed1337/
#   ├── ... (18 directories total)
```

Each directory contains:
- `layer_0.pt` through `layer_21.pt` (22 transformer layers)
- `final_hidden_states.pt`
- `metadata.json`

---

### Phase 4A: Compute Drift Metrics (Results in Git)

**Purpose:** Compute CKA drift metrics (RQ1).

```bash
cd scripts/phase4

# Step 1: Collect performance metrics from trained models
python collect_performance_metrics.py

# Step 2: Compute layer-wise drift (CKA + cosine similarity)
python compute_layer_drift.py \
    --base-dir base_representations \
    --ft-dir results/phase3_representations/representations \
    --output-dir drift_analysis

# Expected outputs (already in git):
# drift_analysis/
#   ├── drift_metrics.csv              # Layer-wise CKA/cosine per model
#   ├── performance_metrics.csv         # Accuracy/F1 per model
#   ├── drift_analysis_results.json    # Summary statistics
#   └── *.png                           # Visualizations
```

**Results in Git:** `drift_analysis/` contains all outputs. You can skip this if not modifying analysis.

---

### Phase 4B: Deployment Benchmarks (Results in Git)

**Purpose:** Measure inference latency (RQ2).

```bash
cd scripts/phase4b

# Run deployment benchmarks (separate vs merged adapters vs Full FT)
python deployment_benchmark.py --output-dir ../../deployment

# Validate multi-adapter correctness
python validate_multi_adapter_correctness.py --output-dir ../../deployment

# Expected outputs (already in git):
# deployment/
#   ├── deployment_benchmark_results.json     # Raw latency data
#   ├── deployment_benchmark_summary.csv      # Summary stats
#   ├── multi_adapter_validation_results.json # Correctness check
#   └── *.png                                  # Visualizations
```

**Results in Git:** `deployment/` contains all outputs. You can skip if not re-benchmarking.

---

### Phase 5: Analysis & Visualizations (In Git)

**Purpose:** Generate final analysis and paper figures.

```bash
# Open Jupyter notebooks
jupyter notebook

# Run these notebooks:
# 1. research_question_1_representational_drift.ipynb  (RQ1 analysis)
# 2. research_question_2_deployment_efficiency.ipynb   (RQ2 analysis)
```

Both notebooks read from `drift_analysis/` and `deployment/` to generate:
- Statistical tests (t-tests, effect sizes, confidence intervals)
- Publication-quality figures
- Summary tables

**All outputs already in git** - notebooks are for transparency/modification.

---

## Project Structure

```
NLP/
├── README.md                          # This file
├── ANALYSIS_REPORT.md                 # Final comprehensive report
├── plan.md                            # Research methodology
├── requirements.txt                   # Python dependencies
│
├── analysis/                          # Optimal hyperparameters (in git)
│   ├── mrpc_full_finetune_optimal.yaml
│   ├── mrpc_lora_optimal.yaml
│   ├── sst2_full_finetune_optimal.yaml
│   ├── sst2_lora_optimal.yaml
│   ├── rte_full_finetune_optimal.yaml
│   └── rte_lora_optimal.yaml
│
├── drift_analysis/                    # RQ1 results (in git)
│   ├── drift_metrics.csv
│   ├── performance_metrics.csv
│   └── *.png
│
├── deployment/                        # RQ2 results (in git)
│   ├── deployment_benchmark_results.json
│   ├── deployment_benchmark_summary.csv
│   └── *.png
│
├── experiments/                       # Training scripts
│   ├── lora_finetune.py               # LoRA training
│   ├── full_finetune.py               # Full FT training
│   ├── optuna_optimization.py         # Hyperparameter search
│   └── baselines.py                   # Baseline metrics
│
├── scripts/                           # Reproduction scripts
│   ├── download_datasets.py           # Phase 0: Download GLUE data
│   ├── extract_base_representations.py  # Extract base representations
│   │
│   ├── phase0/                        # Sanity checks
│   │   └── vm1.sh
│   ├── phase1/                        # Hyperparameter optimization
│   │   └── vm1.sh
│   ├── phase2/                        # Model training
│   │   ├── vm1.sh                     # All tasks
│   │   ├── vm1_full_finetune.sh       # Full FT only
│   │   └── vm1_lora.sh                # LoRA only
│   ├── phase3/                        # Representation extraction
│   │   └── vm1.sh
│   ├── phase4/                        # Drift analysis
│   │   ├── collect_performance_metrics.py
│   │   └── compute_layer_drift.py
│   └── phase4b/                       # Deployment benchmarks
│       ├── deployment_benchmark.py
│       └── validate_multi_adapter_correctness.py
│
├── shared/                            # Core utilities
│   ├── config.yaml                    # Master configuration
│   ├── data_preparation.py            # Dataset loaders
│   ├── metrics.py                     # CKA, cosine similarity
│   └── checkpoint_utils.py            # Training utilities
│
├── research_question_1_representational_drift.ipynb    # RQ1 analysis
└── research_question_2_deployment_efficiency.ipynb     # RQ2 analysis
```

---

## Configuration

All experiments use settings from `shared/config.yaml`:

- **Model:** TinyLlama-1.1B (1.1B parameters, 22 layers)
- **LoRA config:** rank=8, alpha=task-optimized, targets=q_proj+v_proj
- **Tokenization:** max_length=384 (consistent across all phases)
- **Training:** bfloat16, gradient checkpointing, AdamW, cosine schedule
- **Seeds:** 42, 1337, 2024

Optimal hyperparameters per task/method are in `analysis/*.yaml`.

---

## Expected Results

After reproducing all phases, you should obtain:

### RQ1: Representational Drift
| Task  | LoRA Drift | Full FT Drift | Reduction | p-value |
|-------|-----------|---------------|-----------|---------|
| SST-2 | 0.641±0.009 | 0.899±0.018 | **28.6%** | **<0.001** |
| MRPC  | 0.765±0.003 | 0.777±0.003 | 1.6%      | 0.025   |
| RTE   | 0.601±0.003 | 0.602±0.003 | 0.1%      | 0.400   |

### RQ2: Deployment Latency (batch=1, NVIDIA L4)
| Strategy       | Mean Latency | Overhead |
|----------------|-------------|----------|
| Full FT        | 25.5±0.2 ms | Baseline |
| LoRA Merged    | 25.5±0.2 ms | +0.0%    |
| LoRA Separate  | 35.1±0.6 ms | +37.5%   |

### Task Performance
| Task  | Metric   | LoRA         | Full FT      | Advantage |
|-------|----------|-------------|-------------|-----------|
| SST-2 | Accuracy | 95.0±0.4%   | 86.6±3.2%   | +8.4pp    |
| MRPC  | F1       | 88.1±0.2%   | 86.5±3.1%   | +1.6pp    |
| RTE   | Accuracy | 78.7±3.8%   | 56.1±11.5%  | +22.6pp   |

---

## Disk Space Requirements

- **Minimal** (view results only): <1GB
- **With trained models**: ~40GB
- **Full reproduction** (all phases): ~100GB
  - Datasets: ~500MB
  - Trained models: ~36GB
  - Base representations: ~48GB
  - Fine-tuned representations: ~12GB
  - Intermediate files: ~4GB

---

## Hardware & Runtime

**Tested Configuration:**
- GPU: NVIDIA L4 (24GB VRAM)
- CPU: 16 cores
- RAM: 64GB
- OS: Ubuntu 20.04
- CUDA: 11.8
- PyTorch: 2.8.0

**Estimated Runtimes:**
- Phase 0 (datasets + sanity): 1 hour
- Phase 1 (HPO): 4 hours (skip - results in git)
- Phase 2 (train 18 models): 20-24 hours
- Phase 3 (extract representations): 8 hours
- Phase 4A (compute drift): 1 hour
- Phase 4B (deployment benchmarks): 2 hours
- **Total:** ~36 hours (excluding Phase 1)

---

## Troubleshooting

### Out of Memory Errors

```bash
# Reduce batch size in shared/config.yaml
per_device_train_batch_size: 1  # Was 2
gradient_accumulation_steps: 16  # Was 8 (keeps effective batch size)
```

### CUDA Out of Memory

```bash
# Enable CPU offloading for optimizer
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Or use smaller model subset for testing
# Edit scripts to use only 1 seed instead of 3
```

### Reproducibility Issues

Different hardware/CUDA versions may cause minor numerical differences:
- Drift metrics: ±0.01 CKA is normal
- Latency: ±2ms is normal (hardware-dependent)
- Accuracy: ±0.5% across seeds is expected

---

