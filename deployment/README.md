# Deployment Analysis Results

This directory contains deployment benchmark and validation results for Phase 4B (Research Question 2).

**Note**: Unlike most results which are in `results/` (gitignored), these files are committed to git because they're small and important for the analysis notebook to run.

## Files

### `multi_adapter_validation_results.json`
**Validation experiment results** proving that multi-adapter LoRA deployment produces identical predictions to single-adapter deployment.

- **Generated by**: `scripts/phase4b/validate_multi_adapter_correctness.py`
- **Purpose**: Validates the fundamental assumption that multi-adapter overhead comparison is fair (both produce same outputs)
- **Key finding**: âœ… Predictions are 100% identical (bitwise), confirming multi-adapter is functionally equivalent

### `deployment_benchmark_results.json` (to be generated)
**Latency benchmark results** comparing different LoRA deployment strategies:
- Single LoRA adapter (separate)
- LoRA merged adapter
- Full fine-tuned models
- Multi-adapter setups (2 & 3 adapters)

Generated by running: `python scripts/phase4b/deployment_benchmark.py`

### `deployment_analysis_results.json` (optional)
Additional analysis outputs from the deployment benchmarking phase.

## Usage

These files are loaded by `research_question_2_deployment_efficiency.ipynb` to display results and figures.

If you need to regenerate validation results:
```bash
python scripts/phase4b/validate_multi_adapter_correctness.py --num-samples 50
```

If you need to run full deployment benchmarks (requires GPU):
```bash
python scripts/phase4b/deployment_benchmark.py --num-samples 500
```

## Why This Directory?

Deployment analysis results need to be versioned alongside the notebook for reproducibility. The `results/` directory is gitignored for large training artifacts, but these analysis files are small (<10KB) and critical for notebook execution.

